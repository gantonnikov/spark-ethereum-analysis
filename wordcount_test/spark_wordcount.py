import findspark
findspark.init("/opt/homebrew/Cellar/apache-spark/3.5.5/libexec/")

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, split
import time
import psutil
import os

# –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ
print(f"Total CPU Cores Available: {os.cpu_count()}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark (–Ω–∞ 8 —è–¥—Ä–∞—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π)
spark = SparkSession.builder \
    .appName("WordCount_Spark") \
    .config("spark.sql.shuffle.partitions", 8) \
    .master("local[8]") \
    .getOrCreate()

# –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
spark.sparkContext.setLogLevel("ERROR")

# –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤
start_time = time.time()
start_cpu = psutil.cpu_percent()
start_mem = psutil.virtual_memory().percent

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data_path = "large_text_data.txt"  # –§–∞–π–ª –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
df = spark.read.text(data_path).repartition(8)  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏

# –†–µ–∞–ª–∏–∑–∞—Ü–∏—è WordCount
word_count = (
    df.withColumn("word", explode(split(col("value"), r"\s+")))
    .filter(col("word") != "")
    .coalesce(4)  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
    .groupBy("word")
    .count()
    .orderBy("count", ascending=False)
)

# –ü–æ–∫–∞–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
word_count.show(20)

# –ü–æ–¥—Å—á—ë—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
total_rows = df.count()
total_partitions = df.rdd.getNumPartitions()

# –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤
end_time = time.time()
end_cpu = psutil.cpu_percent()
end_mem = psutil.virtual_memory().percent

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print(f"\n‚úÖ Total Rows Processed: {total_rows}")
print(f"‚úÖ Number of Partitions: {total_partitions}")
print(f"‚úÖ Total CPU Cores Used: 8")
print(f"\nüöÄ Execution Time (Spark): {end_time - start_time:.4f} seconds")
print(f"üî• CPU Usage (Spark): {end_cpu}%")
print(f"üíæ RAM Usage (Spark): {end_mem}%")

spark.stop()
