{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c25b66-e7a8-4047-b12d-75f3e6dd3308",
   "metadata": {},
   "source": [
    "# 1. Connection test\n",
    "### Check that Spark connects to BigQuery correctly and loads data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7153ed22-8777-46f0-a106-ef66cb011580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     13\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test from clean env\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.range(2).show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5905360c-a806-4ec9-808c-a47498f4961d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<google.cloud.bigquery.dataset.DatasetListItem object at 0x116055660>, <google.cloud.bigquery.dataset.DatasetListItem object at 0x116057d60>, <google.cloud.bigquery.dataset.DatasetListItem object at 0x116054f10>, <google.cloud.bigquery.dataset.DatasetListItem object at 0x116057400>, <google.cloud.bigquery.dataset.DatasetListItem object at 0x116057160>]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Set the environment variable BEFORE initializing the client\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"spark-access-459420-f8b1f097831d.json\"\n",
    "\n",
    "# Now create the client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# List the first 5 public datasets\n",
    "print(list(client.list_datasets(\"bigquery-public-data\"))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cd91d-7b3c-4a6b-9c83-53b00762baa8",
   "metadata": {},
   "source": [
    "# 2. Batch (Spark) ‚Äî Google BigQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da4b431b-b64a-4a6a-8b53-59090c2031b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ BigQuery...\n",
      "‚ùå –û—à–∏–±–∫–∞: An error occurred while calling o438.load.\n",
      ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: com.google.cloud.spark.bigquery. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
      "\tat org.apache.spark.sql.execution\n",
      "üî¥ –°–µ—Å—Å–∏—è Spark –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# 1. –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏\n",
    "JAR_PATH = \"spark-bigquery-with-dependencies_2.12-0.32.0.jar\"\n",
    "CREDENTIALS = \"spark-access-459420-f8b1f097831d.json\"\n",
    "\n",
    "# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n",
    "if not os.path.exists(JAR_PATH):\n",
    "    print(f\"‚ùå –§–∞–π–ª {JAR_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    exit()\n",
    "if not os.path.exists(CREDENTIALS):\n",
    "    print(f\"‚ùå –§–∞–π–ª {CREDENTIALS} –Ω–µ –Ω–∞–π–¥–µ–Ω!\")\n",
    "    exit()\n",
    "\n",
    "# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EthereumAnalysis\") \\\n",
    "    .config(\"spark.jars\", JAR_PATH) \\\n",
    "    .config(\"spark.driver.extraClassPath\", JAR_PATH) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    print(\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ BigQuery...\")\n",
    "    \n",
    "    # 4. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
    "    df = spark.read.format(\"com.google.cloud.spark.bigquery\") \\\n",
    "        .option(\"table\", \"bigquery-public-data.crypto_ethereum.transactions\") \\\n",
    "        .option(\"filter\", \"block_timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\") \\\n",
    "        .option(\"maxResults\", \"1000\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
    "    print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫: {df.count()}\")\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞: {str(e)[:500]}\")\n",
    "finally:\n",
    "    spark.stop()\n",
    "    print(\"üî¥ –°–µ—Å—Å–∏—è Spark –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c509eee5-31e8-4f1b-ae3a-e1c30f26f8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark-bigquery-with-dependencies_2.12-0.32.0.jar\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext._conf.get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a41b9572-4af0-4745-85d2-6c541aab3134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Spark –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ 0.11 —Å–µ–∫\n",
      "üîπ CPU: 12.8% | RAM: 78.6%\n",
      "\n",
      "üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "‚ùå –û—à–∏–±–∫–∞: An error occurred while calling o292.load.\n",
      ": org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: bigquery. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n",
      "\tat scala.util.Failure.orElse(Try.scala:224)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n",
      "\t... 15 more\n",
      "\n",
      "\n",
      "üî¥ –°–µ—Å—Å–∏—è Spark –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, max, min\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# =============================================\n",
    "# 0. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
    "# =============================================\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"spark-access-459420-f8b1f097831d.json\"\n",
    "\n",
    "# =============================================\n",
    "# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º —Ä–µ—Å—É—Ä—Å–æ–≤\n",
    "# =============================================\n",
    "def init_spark():\n",
    "    start_time = time.time()\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"EthereumAnalysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.32.0\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    init_time = time.time() - start_time\n",
    "    print(f\"üü¢ Spark –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {init_time:.2f} —Å–µ–∫\")\n",
    "    print(f\"üîπ CPU: {psutil.cpu_percent()}% | RAM: {psutil.virtual_memory().percent}%\")\n",
    "    return spark\n",
    "\n",
    "# =============================================\n",
    "# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º\n",
    "# =============================================\n",
    "def load_data(spark):\n",
    "    print(\"\\nüîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π, –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "    df = spark.read.format(\"bigquery\") \\\n",
    "        .option(\"table\", \"bigquery-public-data.crypto_ethereum.transactions\") \\\n",
    "        .option(\"filter\", \"block_timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\") \\\n",
    "        .option(\"maxResults\", \"50000\") \\\n",
    "        .load()\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã ({df.count()} —Å—Ç—Ä–æ–∫, {load_time:.2f} —Å–µ–∫)\")\n",
    "    print(f\"üîπ CPU: {psutil.cpu_percent()}% | RAM: {psutil.virtual_memory().percent}%\")\n",
    "    return df\n",
    "\n",
    "# =============================================\n",
    "# 3. –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö\n",
    "# =============================================\n",
    "def analyze_data(df):\n",
    "    print(\"\\nüìä –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    \n",
    "    # –¢–æ–ø-10 –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ—à–µ–ª—å–∫–æ–≤\n",
    "    print(\"\\nüîù –¢–æ–ø-10 –∫–æ—à–µ–ª—å–∫–æ–≤ –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:\")\n",
    "    top_wallets = df.groupBy(\"from_address\") \\\n",
    "        .agg(count(\"*\").alias(\"tx_count\")) \\\n",
    "        .orderBy(col(\"tx_count\").desc()) \\\n",
    "        .limit(10)\n",
    "    top_wallets.show(truncate=False)\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è–º\n",
    "    stats = df.agg(\n",
    "        count(\"*\").alias(\"total_tx\"),\n",
    "        min(\"value\").alias(\"min_value\"),\n",
    "        max(\"value\").alias(\"max_value\"),\n",
    "        sum(\"value\").alias(\"total_value\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"\\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "    print(f\"‚Ä¢ –í—Å–µ–≥–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {stats['total_tx']}\")\n",
    "    print(f\"‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞: {stats['min_value']} wei\")\n",
    "    print(f\"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å—É–º–º–∞: {stats['max_value']} wei\")\n",
    "    print(f\"‚Ä¢ –û–±—â–∏–π –æ–±—ä–µ–º: {stats['total_value']} wei\")\n",
    "\n",
    "    return top_wallets, stats\n",
    "\n",
    "# =============================================\n",
    "# 4. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–æ—à–µ–ª—å–∫–æ–≤\n",
    "# =============================================\n",
    "def cluster_wallets(df):\n",
    "    print(\"\\nüîÆ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∫–æ—à–µ–ª—å–∫–æ–≤...\")\n",
    "    \n",
    "    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–æ—à–µ–ª—å–∫–∞–º\n",
    "    wallet_stats = df.groupBy(\"from_address\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"tx_count\"),\n",
    "            sum(\"value\").alias(\"total_value\")\n",
    "        )\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"tx_count\", \"total_value\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    features = assembler.transform(wallet_stats)\n",
    "    \n",
    "    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means\n",
    "    kmeans = KMeans(k=3, seed=42)\n",
    "    model = kmeans.fit(features)\n",
    "    clustered = model.transform(features)\n",
    "    \n",
    "    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    clusters = clustered.groupBy(\"prediction\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"wallets_count\"),\n",
    "            avg(\"tx_count\").alias(\"avg_tx_count\"),\n",
    "            avg(\"total_value\").alias(\"avg_value\")\n",
    "        ) \\\n",
    "        .orderBy(\"prediction\")\n",
    "    \n",
    "    print(\"\\nüìå –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:\")\n",
    "    clusters.show()\n",
    "    \n",
    "    return clustered\n",
    "\n",
    "# =============================================\n",
    "# 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "# =============================================\n",
    "def visualize_data(df, top_wallets):\n",
    "    print(\"\\nüé® –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    \n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Pandas –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "    sample = df.sample(0.1).limit(1000).toPandas()\n",
    "    top_wallets_pd = top_wallets.toPandas()\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 1: –¢–æ–ø-10 –∫–æ—à–µ–ª—å–∫–æ–≤\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(\n",
    "        top_wallets_pd[\"from_address\"].str[:10] + \"...\", \n",
    "        top_wallets_pd[\"tx_count\"],\n",
    "        color='skyblue'\n",
    "    )\n",
    "    plt.title(\"–¢–æ–ø-10 –∞–∫—Ç–∏–≤–Ω—ã—Ö –∫–æ—à–µ–ª—å–∫–æ–≤\")\n",
    "    plt.xlabel(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"top_wallets.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–º–º\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(sample[\"value\"].astype(float), bins=50, log=True, color='green')\n",
    "    plt.title(\"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—É–º–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π (log scale)\")\n",
    "    plt.xlabel(\"–°—É–º–º–∞ (wei)\")\n",
    "    plt.ylabel(\"–ß–∞—Å—Ç–æ—Ç–∞\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"tx_distribution.png\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================\n",
    "# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\n",
    "# =============================================\n",
    "def main():\n",
    "    try:\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        spark = init_spark()\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "        df = load_data(spark)\n",
    "        \n",
    "        # –ê–Ω–∞–ª–∏–∑\n",
    "        top_wallets, stats = analyze_data(df)\n",
    "        \n",
    "        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–æ)\n",
    "        # clustered = cluster_wallets(df)\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        visualize_data(df, top_wallets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞: {e}\")\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        print(\"\\nüî¥ –°–µ—Å—Å–∏—è Spark –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36445644-eb1e-445e-84c2-4d9a99075cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
